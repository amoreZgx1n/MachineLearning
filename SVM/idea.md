支持向量机（SVM）在神经网络出现之前内受青睐，由于使用的数学理论非常巧妙，并且这些数学理论很完善，具有可解释性强的特点SVM也是建模以及优化的优秀例子，值得学习。
SVM三宝：间隔对偶核技巧

具体来讲，我们为了保证分类器更好的分类效果，采用间隔评判分类器的分类能力；对偶是SVM最优化过程中重要步骤和思想，非常巧妙，也成为优化问题的重要思路；核技巧则是将SVM拓展到非线性问题，利用卷积核制造的维度差解决问题。

## 硬间隔分类

### 两个概念

对于线性分类器，首先需要简单了解一下一些概念的数学定义

#### 线性可分

SVM通常来解决线性可分问题，线性可分的定义：
$D_0$ 和 $D_1$ 是 n 维欧氏空间中的两个点集。如果存在 n 维向量 w 和实数 b，使得所有属于 $D_0$ 的点 $x_i$ 都有 $wx_i+b>0$ ，而对于所有属于 $D_1$ 的点 $x_j$ 则有 $wx_i+b<0$ ，则我们称 $D_0$ 和 $D_0$ 线性可分。
直观地说，在一个棋盘（二维空间），可以划一条线将棋子分开；对一个蛋糕（三维平面），可以切一刀将蛋糕分成两部分；高维空间同理。
$wx_i+b=0$就形成了一个超平面

#### 最大分类超平面

如下图1，一个二维平面上有一群点，我们要一条线将其分为两部分，显然有无数条线符合条件，我们要选取其中的最大间隔的直线 ，在这里是$H_3$

<img src="https://amore.oss-cn-hangzhou.aliyuncs.com/img/svm1.png#x" style="zoom: 33%;" />

推广到多维空间也一样，只不过找的是一个最优超平面：

1. 两类样本分别在该超平面的两侧
2. 两侧距离超平面最近的样本点到超平面的距离最大

### SVM最优化

SVM是至今学习的优化算法的比较难的，因为它用到的数学知识比较多，有些还没有学过，但是这些优化方法十分常见，学会这些优化方法也是学习SVM的收获。

#### 间隔

间隔是SVM三大关键的第一个，是SVM的思路来源，原理很简单，是在感知机基础上的改进版本，比较理解即可

##### 优化目标的导出

对于二维平面中直线 $Ax+By+C=0$，点(x,y)到直线距离公式：
$$D=\frac{|A x+B y+C|}{\sqrt{A^{2}+B^{2}}}$$
相应地，在n维空间内，点($x_1$,$x_2$,...,$x_n$)到直线$wx_i+b=0$距离为
$$D=\frac{\left|w^{T} x+b\right|}{\|w\|_2}$$ 
我们因此得到分段函数：
$$
\left\lbrace\begin{array}{l}
\frac{w^{T} x+b}{\|w\|} \geq d \quad y=1 \\\\
\frac{w^{T} x+b}{\|w\|} \leq-d \quad y=-1
\end{array}\right.
$$ {1}
移项得：
$$
\left\lbrace\begin{array}{l}
\frac{w^{T} x+b}{\|w\| d} \geq 1 \quad y=1 \\\\
\frac{w^{T} x+b}{\|w\| d} \leq-1 \quad y=-1
\end{array}\right.
$$
因为 $\|w\| d$ 是正数，所以我们令它等于1，相当于将间隔放缩，不会影响到结果**#1**，并且可以简化运算:
$$
\left\lbrace\begin{array}{l}
w^{T} x+b \geq 1 \quad y=1 \\
w^{T} x+b \leq-1 \quad y=-1
\end{array}\right.
$$
等价于
$$
y(w^{T} x+b) \geq 1
$$
需要注意这个式子的意义是正确分类的条件，我们因此也得到了超平面，如下图2（图片来源：Bing图片）：

<img src="https://amore.oss-cn-hangzhou.aliyuncs.com/img/SVM2.png" style="zoom: 50%;" />

对于间隔分类我的简单理解：对于任意样本点，对应1或者-1的标签，优化目标成立的前提是**做到正确分类**，所以 $y(w^{T} x+b) \geq 1$是限制条件；优化目标 $\gamma$也很容易理解，即图2中两条直线的距离，很多教程和书籍对**#1**放缩没有给出直观的解释，这两条超平面只是一个分类标准，假如改为 $y(w^{T} x+b) \geq $2，那么结果是将两条直线向外扩，在图2中显然需要有样本点无法判别分类，但是我们知道y的正负表示类别，数值可以放缩，y放缩之后样本点在该超平面分类下的效果是一样的。一句话，超平面保证正确分类即可。

专业点的解释：$|w^{T}x+b|$相对地表示点x到超平面的距离远近，反应分类预测的确信程度，而 $x^{T}x+b$的符号与y的符号的一致性表示分类是否正确，因此 $y(w^{T}x+b)$表示分类的正确性以及确信度。

##### 函数间隔 vs 几何间隔

**函数间隔**

对于给定的训练数据集T和超平面 $(w, b)$， 定义超平面 $(w, b)$关于样本 $(x^i, y^i)$的函数间隔为：
$$
\hat{\gamma}_{i}=y i(w \cdot x i+b)
$$
定义超平面 $(w, b)$关于样本数据集T的函数间隔为：
$$
\hat{\gamma}=\min _{i=1, \cdots, N} \hat{\gamma}_{i}
$$
即所有  $\hat{\gamma}$的最小值

**几何间隔**

对于给定的训练数据集T和超平面 $(w, b)$， 定义超平面 $(w, b)$关于样本 $(x^i, y^i)$的函数间隔为：
$$
\gamma^{i}=y^{i}\left(\frac{w}{\|w\|} \cdot x^{i}+\frac{b}{\|w\|}\right)
$$
定义超平面 $(w, b)$关于样本数据集T的函数间隔为：
$$
\gamma=\min _{i=1, \cdots, N} \hat{\gamma}_{i}
$$
即所有  $\gamma$的最小值

**二者比较**

观察两式的区别仅仅是后者进行了归一化。上面**#1**的放缩问题已经解释过，使用函数间隔时只是为了正确分类，所以距离表示分类预测正确的准确度，当w和b成比例改变时，超平面没有改变，但函数间隔改变（上面提到的超平面外扩例子）。使用几何间隔进行改进，当w和b成比例改变时，几何间隔不会改变。

所以，函数间隔与几何间隔的关系：
$$
\text { 几何间隔 } \gamma=\frac{\text { 函数间隔 } \hat{\gamma}}{\|w\|}
$$
当 $||w||=1$时，函数距离 = 几何距离

**深入理解**

几何间隔之所以可以到达这个效果，因为几何间隔的距离（注意距离和间隔的区别）就是点到直线的距离

简单推导一下：对于支持向量B所在的超平面 $w^Tx_i+b=0$，对任意的点 $A(x_i, y_i)$

#### 最优化求解

使用常规的拉格朗日乘子法进行求解，不过需要先进行对偶处理，对偶算法是最优化方法常见的方法（对偶理论很复杂，我们只简单提取用于最优化的部分）