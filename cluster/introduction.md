## 聚类

聚类是对于未标注的数据集，按照数据的内在相似性将数据集进行划分，形成多个簇，使得内部数据相似度尽量大而类别间的数据相似度尽可能小。是一种常用的非监督学习方法。

### 简单介绍

聚类会使用很多种常用的距离度量方法，使用范数进行表示，具体意义详见范式的讲解，这里不再赘述；聚类在不同场景下也会使用不同种类的算法思想，主要的类别有三种。

#### 相似度/距离度量

使用范数来表示距离，`Minkowski​` 距离是$L_p$范数，`Manhattan` 距离、`Euclidean`距离、`Chebyshev`距离分别对应  $p = 1, 2, \infty$时的情形。如下图：

<img src="https://amore.oss-cn-hangzhou.aliyuncs.com/img/20220715201001.png" style="zoom: 50%;" />

#### 聚类方法

常用的聚类思路有三类，有划分式聚类方法、基于密度聚类方法、层次化聚类方法，这三种聚类方法是我们面对聚类任务的时候首先想到的，我们也会从每种思路种选取2~3种方法进行介绍，部分聚类方法如下图：

<img src="https://amore.oss-cn-hangzhou.aliyuncs.com/img/20220715202929.png" style="zoom:50%;" />

关于Deep Clustering，在NLP等特定领域会进行深度学习和聚类的结合，方向有基于AE、VAE和GAN三种，但一般传统的聚类方法就有很好的效果。另外当需要做时间序列的特征提取的时候，还会用到时序聚类，主要分为两步：时间序列的特征提取和时间序列的相似度计算。

### 划分式聚类方法

划分式聚类方法**需要事先指定簇类的数目或者聚类中心**，通过反复迭代，直至最后达到**簇内的点足够近，簇间的点足够远**的目标。经典的划分式聚类方法有`k-means`及其变体`k-means++`、`bi-kmeans`、`kernel k-means`等。

#### k-means

k-means的三个条件：平面数值坐标、密度均匀、球状分布，特点是对初始簇中心和异常值敏感 、需要提前确定k值

思路：while{确定簇中心、簇分配}->直到簇中心不变为止 

优点：简单、局部最优但是易用、复杂度低:  $O(nkt) $，最好簇中心方便确定

下图是选取自西瓜书的k-means基本步骤：

<img src="https://amore.oss-cn-hangzhou.aliyuncs.com/img/20220716064157.png" style="zoom:50%;" />

这个算法看起来很顺利，但是我们需要证明其局部最优问题，对于初始化的均值向量$\left\{\boldsymbol{\mu}_{1}, \boldsymbol{\mu}_{2}, \ldots, \boldsymbol{\mu}_{k}\right\}$和样本集$\left\{\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{m}\right\}$，k-means的优化目标
$$
\min \sum_{i=1}^{m} \min _{j=1,2, \ldots, k}\left\|x_{i}-\mu_{j}\right\|^{2}
$$
优化函数 $\min _{j=1,2, \ldots, k}\left\|x_{i}-\mu_{j}\right\|^{2}$是非凸优化函数，会收敛于局部最优解，直观解释是一组凸函数取min，一定是非凸的。证明过程还不清楚，但是我从两个例子解释优化函数的非凸性：

1. 一维情况下，令两个中心向量$\mu_{1}$、$\mu_{2}$分别为 $1$和$2$，使用`wolframalpha`绘图结果如下（标准版图片比较模糊）：

   ![](https://amore.oss-cn-hangzhou.aliyuncs.com/img/20220716080031.png)

2. 二维情况下，令两个中心向量$\mu_{1}$、$\mu_{2}$分别为 $[1, 1]$和$[1, -1]$，使用软件绘图结果如下：

   显然是非凸函数

另外，这个问题有点像图理论的NP-hard k-center问题，可以对比进行理解。需要注意的是，不同的局部最优解得到的中心点的聚类结果是不同的。

最后讨论下k-means的实质。经过以上对优化函数的讨论，优化函数的自然语言描述是使类间方差最小，即类间样本点的差异（到中心点距离）最小。

#### k-means++

`k-means++`是针对`k-means`中初始质心点选取的优化算法。该算法的流程和`k-means`类似，改变的地方只有初始质心的选取，初始质心选取步骤如下

1. 随机选取一个数据点作为初始的聚类中心

2. 当聚类中心数量小于k：

   计算每个数据点与当前已有聚类中心的最短距离，用$D(z)$表示，这个值越大，表示被选取为下一个聚类中心的概率越大，最后使用轮盘法选取下一个聚类中心

*轮盘法：模拟转盘的选取方式，各个个体的选择概率和其适应度值成比例，适应度越大，选中概率也越大。轮盘法的关键是计算累积概率，具体解释可以到“ohter”笔记中查看，里面有代码实现*

#### bi-kmeans

一开始介绍聚类的时候，我们就表明聚类的目标是类间差异大、类内差异小。一种度量聚类效果的指标是`SSE(Sum of Squared Error)`，表示聚类后的簇离该簇的聚类中心的平方和，也就是上面讨论k-means的实质。而所求方差依赖中心点选择，而k-means局部最优的情况导致中心点有多种选择。`bi-kmeans`是针对`kmeans`算法会陷入局部最优的缺陷进行的改进算法，还解决了初始化k个随机的质心点时其中一个或者多个点由于位置太极端而导致迭代的过程中消失的问题。该算法基于SSE最小化的原理，首先将所有的数据点视为一个簇，然后将该簇一分为二，之后选择其中一个簇继续进行划分，选择哪一个簇进行划分取决于对其划分是否能最大程度的降低`SSE`的值。

算法流程如下：

1. 将所有点视为一个簇 

2. 当簇的个数小于k时
   对每一个簇：

   1. 计算总误差
   2. 在给定的簇上面进行k-means聚类(k=2)
   3. 计算将该簇一分为二之后的总误差

   选取使得误差最小的那个簇进行划分操作

这是一个全局最优的方法，所以每次计算出来的`SSE`值基本也是一样的。bi-kmeans的思想和决策树很像，可以结合决策树来理解。

当然k-means的改进还有很多，比如基于大数据用以减少运算时间的MiniBatch k-means、映射到高维空间而更灵活的Kernel k-means等等。

### 基于密度聚类方法



### 层次化聚类方法



### 聚类结果评价





